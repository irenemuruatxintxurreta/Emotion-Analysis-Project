{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DnOLWNAvlk26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a67d18-3559-4e58-d86d-b19b68c31282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade fsspec datasets\n"
      ],
      "metadata": {
        "id": "_SE_64vlOvQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e146f211-225a-4b9e-bac0-e6973d509e88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Acquisition ‚Äì GoEmotions.\n",
        "\n",
        "For this project, I used the GoEmotions dataset, developed by Google and available via the HuggingFace Datasets library. This dataset contains over 58,000 English sentences annotated with emotion labels, making it ideal for training and evaluating emotion classification models.\n",
        "\n",
        "Unlike basic sentiment analysis (positive/negative/neutral), GoEmotions includes 27 fine-grained emotions plus a neutral label. It also provides mappings to Ekman's 6 basic emotions (joy, sadness, anger, fear, disgust, surprise) for simpler use cases.\n",
        "\n",
        "‚úÖ Key Steps:\n",
        "Loaded the dataset using load_dataset(\"go_emotions\") from HuggingFace\n",
        "\n",
        "Explored the dataset structure: it includes train, validation, and test splits\n",
        "\n",
        "Each example contains:\n",
        "\n",
        "A sentence of text (e.g., \"I'm feeling really down today\")\n",
        "\n",
        "One or more emotion labels represented by integers\n",
        "\n",
        "This dataset forms the foundation for training the machine learning model to recognize emotions from written text.\n"
      ],
      "metadata": {
        "id": "iNu98DVEo3oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"go_emotions\")\n",
        "\n",
        "# Print example\n",
        "print(dataset['train'][0])\n"
      ],
      "metadata": {
        "id": "ApBPGY7nmJS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "768ef536-cfe8-4837-cbab-b34faae19663"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"My favourite food is anything I didn't have to cook myself.\", 'labels': [27], 'id': 'eebbqej'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset['train'].features['labels'].feature.names\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "MEVbJJFvoyzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9259d042-5836-428d-cf4c-07f25888dc38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing\n",
        "\n",
        "Preprocessing is a crucial step in preparing raw text for machine learning. Different models have different needs when it comes to input text, so this step varies depending on the type of model used.\n",
        "\n",
        "üü© Preprocessing for Traditional Machine Learning Models\n",
        "Traditional models like Support Vector Machines (SVM) or Random Forests require numerical input. So, we need to clean the text and convert it into a format the model can understand.\n",
        "\n",
        "The preprocessing includes:\n",
        "\n",
        "Lowercasing the text (to treat ‚ÄúHappy‚Äù and ‚Äúhappy‚Äù the same)\n",
        "\n",
        "Removing punctuation, numbers, and special characters\n",
        "\n",
        "Tokenizing: splitting the sentence into words\n",
        "\n",
        "Removing stop words (common words like ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúand‚Äù that add little meaning)"
      ],
      "metadata": {
        "id": "5DRJfju2pB0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab for tokenization\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation and numbers\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "oPZnK-GwrJem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e7cd8d-6f04-481e-d094-b30f2e42e9f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = dataset['train'][0]['text']\n",
        "print(\"Original:\", sample_text)\n",
        "print(\"Processed:\", preprocess_text(sample_text))\n"
      ],
      "metadata": {
        "id": "TLCwvNNBrKdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d212131-f027-44ce-904a-4a0d1312de5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: My favourite food is anything I didn't have to cook myself.\n",
            "Processed: favourite food anything didnt cook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering and Model Development (Traditional ML)\n",
        "\n",
        "In this step, we‚Äôll turn the cleaned text data into a format that the machine learning model can understand and learn from. Specifically, we‚Äôll convert the text into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency). Then, we‚Äôll train a traditional model, like SVM (Support Vector Machine) or Random Forest, on that data.\n",
        "\n",
        "üßë‚Äçüè´ What is TF-IDF?\n",
        "TF (Term Frequency) tells you how often a word appears in a document (sentence).\n",
        "\n",
        "IDF (Inverse Document Frequency) tells you how important a word is in the entire dataset (text corpus).\n",
        "\n",
        "So, TF-IDF gives each word a weight based on:\n",
        "\n",
        "How often it appears in the sentence\n",
        "\n",
        "How unique it is across the whole dataset\n",
        "\n",
        "This makes important words stand out more and less meaningful words (like ‚Äúthe‚Äù or ‚Äúand‚Äù) get less weight."
      ],
      "metadata": {
        "id": "m2HAQJI7td7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ Steps for Feature Extraction (TF-IDF) and Model Training\n",
        "Convert Text to Numerical Features with TF-IDF\n",
        "\n",
        "We'll use TfidfVectorizer from scikit-learn to do this.\n",
        "\n",
        "Train a Traditional ML Model\n",
        "\n",
        "We‚Äôll train a classifier, like SVM, on the TF-IDF features."
      ],
      "metadata": {
        "id": "5i57GJnuti6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import hamming_loss, f1_score, classification_report\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Step 1: Preprocess the text\n",
        "# Apply preprocessing function to the dataset\n",
        "processed_texts = [preprocess_text(text) for text in dataset['train']['text']]\n",
        "\n",
        "# Step 2: Convert text to numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to 5000 features for simplicity\n",
        "X = vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "# Step 3: Extract labels as a 1D array\n",
        "y = dataset['train']['labels']\n",
        "\n",
        "# Convert multi-labels to single-label using MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(y)\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train a Support Vector Machine (SVM) model\n",
        "# Use OneVsRestClassifier for multi-label classification\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "model = OneVsRestClassifier(SVC(kernel='linear'))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Output accuracy and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n",
        "print(\"Micro F1-score:\", f1_score(y_test, y_pred, average='micro'))\n",
        "print(\"Macro F1-score:\", f1_score(y_test, y_pred, average='macro'))"
      ],
      "metadata": {
        "id": "2aHffeDxrjRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf00715-0fd7-4d6f-b2cf-b120f2831290"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.32688320663441606\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.45      0.57       863\n",
            "           1       0.77      0.71      0.74       453\n",
            "           2       0.71      0.15      0.25       323\n",
            "           3       0.63      0.02      0.05       483\n",
            "           4       0.72      0.07      0.13       577\n",
            "           5       0.75      0.03      0.05       212\n",
            "           6       0.87      0.05      0.10       258\n",
            "           7       0.95      0.08      0.14       460\n",
            "           8       0.71      0.27      0.40       128\n",
            "           9       0.92      0.05      0.09       244\n",
            "          10       0.50      0.01      0.01       383\n",
            "          11       0.86      0.19      0.31       156\n",
            "          12       0.87      0.22      0.36        58\n",
            "          13       0.74      0.11      0.20       175\n",
            "          14       0.74      0.32      0.45       116\n",
            "          15       0.97      0.83      0.90       544\n",
            "          16       0.00      0.00      0.00        14\n",
            "          17       0.65      0.30      0.41       314\n",
            "          18       0.76      0.70      0.73       437\n",
            "          19       1.00      0.06      0.11        35\n",
            "          20       0.68      0.38      0.48       324\n",
            "          21       0.50      0.17      0.25        24\n",
            "          22       0.80      0.07      0.13       223\n",
            "          23       0.00      0.00      0.00        27\n",
            "          24       0.58      0.39      0.47       110\n",
            "          25       0.81      0.25      0.38       276\n",
            "          26       0.76      0.20      0.32       205\n",
            "          27       0.60      0.50      0.54      2812\n",
            "\n",
            "   micro avg       0.70      0.35      0.46     10234\n",
            "   macro avg       0.70      0.24      0.31     10234\n",
            "weighted avg       0.72      0.35      0.41     10234\n",
            " samples avg       0.39      0.37      0.37     10234\n",
            "\n",
            "Hamming Loss: 0.03375621153782868\n",
            "Micro F1-score: 0.4647795460474824\n",
            "Macro F1-score: 0.30570776539536826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "id": "xfoL0rxEIi-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03723a4e-757b-40c8-a9aa-f4b4e3ccfca0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount  Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# 2. Create a folder in  Drive to hold the artifacts\n",
        "!mkdir -p /content/drive/MyDrive/svm_emotion_baseline\n",
        "\n",
        "# 3. Dump the objects\n",
        "joblib.dump(vectorizer, \"/content/drive/MyDrive/svm_emotion_baseline/tfidf_vectorizer.joblib\")\n",
        "joblib.dump(model,      \"/content/drive/MyDrive/svm_emotion_baseline/svm_model.joblib\")\n",
        "joblib.dump(mlb,        \"/content/drive/MyDrive/svm_emotion_baseline/label_binarizer.joblib\")\n",
        "\n",
        "# 4. Verify\n",
        "!ls -1 /content/drive/MyDrive/svm_emotion_baseline\n"
      ],
      "metadata": {
        "id": "psvjMKS_KAcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e348f0-1021-4bcf-cdde-ca073a29ce32"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "label_binarizer.joblib\n",
            "svm_model.joblib\n",
            "tfidf_vectorizer.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù Baseline Model Results Summary (TF-IDF + SVM)\n",
        "For the initial stage of the project, we implemented a baseline machine learning model using a TF-IDF vectorizer combined with a Support Vector Machine (SVM) in a multi-label classification setup. The goal was to establish a performance benchmark before moving on to more complex models like BERT.\n",
        "\n",
        "‚öôÔ∏è Model Setup:\n",
        "Feature extraction: TF-IDF (max_features=5000)\n",
        "\n",
        "Classifier: SVM with linear kernel, wrapped in a OneVsRestClassifier for multi-label support\n",
        "\n",
        "Label binarization: MultiLabelBinarizer to handle multiple emotion labels per text sample\n",
        "\n",
        "üìä Evaluation Metrics:\n",
        "\n",
        "Metric\tScore\n",
        "Hamming Loss\t0.033\n",
        "Micro F1-score\t0.464\n",
        "Macro F1-score\t0.304\n",
        "\n",
        "üîç Interpretation:\n",
        "\n",
        "Low Hamming Loss (0.033) indicates that, on average, very few labels were incorrectly predicted per sample.\n",
        "\n",
        "Micro F1-score of 0.464 shows good performance on frequent emotions, which dominate the dataset.\n",
        "\n",
        "Macro F1-score of 0.304 is expected to be lower due to the imbalanced distribution of emotions in the dataset ‚Äî rarer emotions are harder for the model to learn.\n",
        "\n",
        "üß† Insights:\n",
        "This model provides a strong baseline, especially considering it was built using simple feature extraction and traditional machine learning. However, it has limited understanding of context and nuance, which are critical for accurately detecting emotions in natural language. Therefore, we will now move on to a deep learning approach, leveraging a pre-trained language model (BERT), to improve emotion classification performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "wh4xoIbB2kjK"
      }
    }
  ]
}